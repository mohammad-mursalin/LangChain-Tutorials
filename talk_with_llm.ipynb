{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cba7393",
   "metadata": {},
   "source": [
    "# Chat Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6e679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60211898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    " \n",
    "deepseekChatModel = ChatGroq( model = \"llama-3.1-8b-instant\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b59de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To connect to Gemini 2.0 Flash using LangChain, you'll need to use the LangChain `LlamaClient` with the Gemini 2.0 Flash API endpoint.\n",
      "\n",
      "Here's a step-by-step guide to get you started:\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "* Install LangChain using npm or yarn: `npm install langchain` or `yarn add langchain`\n",
      "* Make sure you have a Gemini 2.0 Flash account and have obtained an API key\n",
      "\n",
      "### Step 1: Create a LangChain `LlamaClient` instance\n",
      "\n",
      "```javascript\n",
      "import { LlamaClient } from 'langchain';\n",
      "import { GeminiClient } from 'langchain/clients/gemini';\n",
      "\n",
      "const apiEndpoint = 'https://api.flash.gemini.ai/v2/flash'; // Replace with your Gemini 2.0 Flash API endpoint\n",
      "const apiKey = 'YOUR_API_KEY_HERE'; // Replace with your Gemini 2.0 Flash API key\n",
      "\n",
      "const geminiClient = new GeminiClient({\n",
      "  apiEndpoint,\n",
      "  apiKey,\n",
      "});\n",
      "```\n",
      "\n",
      "### Step 2: Create a LangChain `LLoRaClient` instance\n",
      "\n",
      "```javascript\n",
      "const lloraClient = new LlamaClient({\n",
      "  client: geminiClient,\n",
      "  temperature: 0, // Optional, default is 0\n",
      "  maxTokens: 2048, // Optional, default is 2048\n",
      "});\n",
      "```\n",
      "\n",
      "### Step 3: Use the `lloraClient` to send requests to Gemini 2.0 Flash\n",
      "\n",
      "```javascript\n",
      "const question = 'What is the capital of France?';\n",
      "const response = await lloraClient.ask(question);\n",
      "\n",
      "console.log(response.result);\n",
      "```\n",
      "\n",
      "### Example Use Case\n",
      "\n",
      "Here's a complete example that demonstrates how to use LangChain to connect to Gemini 2.0 Flash and ask a question:\n",
      "\n",
      "```javascript\n",
      "import { LlamaClient } from 'langchain';\n",
      "import { GeminiClient } from 'langchain/clients/gemini';\n",
      "\n",
      "const apiEndpoint = 'https://api.flash.gemini.ai/v2/flash';\n",
      "const apiKey = 'YOUR_API_KEY_HERE';\n",
      "\n",
      "const geminiClient = new GeminiClient({\n",
      "  apiEndpoint,\n",
      "  apiKey,\n",
      "});\n",
      "\n",
      "const lloraClient = new LlamaClient({\n",
      "  client: geminiClient,\n",
      "  temperature: 0,\n",
      "  maxTokens: 2048,\n",
      "});\n",
      "\n",
      "const question = 'What is the capital of France?';\n",
      "const response = await lloraClient.ask(question);\n",
      "\n",
      "console.log(response.result);\n",
      "```\n",
      "\n",
      "Make sure to replace `YOUR_API_KEY_HERE` with your actual Gemini 2.0 Flash API key.\n",
      "\n",
      "### Tips\n",
      "\n",
      "* Make sure to handle errors and exceptions properly when using the `lloraClient`.\n",
      "* Adjust the `temperature` and `maxTokens` settings to suit your specific use case.\n",
      "* Refer to the LangChain documentation and the Gemini 2.0 Flash API documentation for more information on usage and configuration.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    (\"system\", \"you are a programmer\"),\n",
    "    (\"human\", \"how can i connect to gemini 2.0 flash using langchain\")\n",
    "]\n",
    "\n",
    "response = deepseekChatModel.invoke(message)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
