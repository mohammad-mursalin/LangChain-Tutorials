{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# How to build an advanced Chatbot with session memory using LangChain\n",
    "* Advanced Chatbot LLM App.\n",
    "    * Will be able to have a conversation.\n",
    "    * Will remember previous interactions: will have memory.\n",
    "    * Will be able to have different memories for different user sessions.\n",
    "    * Will be able to remember a limited number of messages: limited memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0399355-dece-4701-9bf4-4c204fe74929",
   "metadata": {},
   "source": [
    "## Concepts included\n",
    "* Chat Model vs. LLM Model:\n",
    "    *  Chat Model is based around messages.\n",
    "    *  LLM Model is based around raw text.\n",
    "* Chat History: allows Chat Model to remember previous interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67075f4a-6c25-41db-b851-51df80ffd927",
   "metadata": {},
   "source": [
    "## Trick to avoid the nasty deprecation warnings from LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49577308-4ed9-4c4b-a59a-a825b33ea75d",
   "metadata": {},
   "source": [
    "In this exercise we will use the LangChain legacy chain LLMChain. It works well, but LangChain displays a nasty deprecation warning. To avoid it, we will enter the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157958ab-f806-4ced-9a9a-bd22cf61e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bf0fd-0661-4350-8979-53266845c8dd",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b08934-b7f0-4e8a-b5f7-9df3c8bdbc66",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4cec3-cc60-49f1-a24e-793b3be7cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5ddc8-1f67-4efc-b26f-5a7bfb8fda5c",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and start a conversation with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646540d-6dae-468c-b5e0-5348106d034b",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "* For this project, we will use OpenAI's gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chatbot = ChatGroq(model = \"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1d409-7f2f-40a4-90c5-ad77dad3edce",
   "metadata": {},
   "source": [
    "* Human Message: the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dc0613-fb6d-4c82-a614-9e7307714303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messagesToTheChatbot = [\n",
    "    HumanMessage(content=\"My favourite color is blue.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b36ec-341a-47bc-af32-30cfb939810d",
   "metadata": {},
   "source": [
    "#### Call the ChatModel (the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db815e32-8e60-46b3-8cce-fbf40e378397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's a popular favorite color. Blue is often associated with feelings of calmness, serenity, and trust. Many people find the color blue to be soothing and uplifting. What shade of blue is your favorite, by the way? Are you more of a light sky blue, a deep navy blue, or somewhere in between?\", response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 41, 'total_tokens': 108, 'completion_time': 0.107578766, 'prompt_time': 0.001925152, 'queue_time': 0.050103067, 'total_time': 0.109503918}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-74528d03-ef2d-4ce1-9cac-e0133c00c15e-0', usage_metadata={'input_tokens': 41, 'output_tokens': 67, 'total_tokens': 108})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.invoke(messagesToTheChatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913fc8c-254f-410d-aa8f-35eb0898855e",
   "metadata": {},
   "source": [
    "#### Track the operation in LangSmith\n",
    "* [Open LangSmith here](smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62b67b-9798-4705-8b8a-dbfdf8c93ed8",
   "metadata": {},
   "source": [
    "## Check if the Chatbot remembers your favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ead4ee-bda3-4c52-9bdb-7bf234733055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your preferences, including your favorite color. This conversation just started, and I'm here to provide information or help with a specific question. If you'd like to share your favorite color, I'd be happy to chat with you about it.\", response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 41, 'total_tokens': 97, 'completion_time': 0.063694568, 'prompt_time': 0.002418423, 'queue_time': 0.050587325, 'total_time': 0.066112991}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-c6ee9e8c-e582-42e1-84e0-48c4388ceb9c-0', usage_metadata={'input_tokens': 41, 'output_tokens': 56, 'total_tokens': 97})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.invoke([\n",
    "    HumanMessage(content=\"What is my favorite color?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec947e-b9b7-43e3-ac67-60027e049f3c",
   "metadata": {},
   "source": [
    "* As you can see, our Chatbot cannot remember our previous interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0143e-8bc8-45ad-ac6f-05aaf659c683",
   "metadata": {},
   "source": [
    "## Let's add memory to our Chatbot\n",
    "* We will use the ChatMessageHistory package.\n",
    "* We will save the Chatbot memory in a python dictionary called chatbotMemory.\n",
    "* We will define the get_session_history function to create a session_id for each conversation.\n",
    "* We will use the built-in runnable RunnableWithMesageHistory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2680fc-8b7a-429f-a570-3c3dc4681037",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23533a37-6060-4d32-9b0c-36a9ea57a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c1d395-0656-46f4-a14e-70cd3e30e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chatbotMemory = {}\n",
    "\n",
    "# input: session_id, output: chatbotMemory[session_id]\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chatbotMemory:\n",
    "        chatbotMemory[session_id] = ChatMessageHistory()\n",
    "    return chatbotMemory[session_id]\n",
    "\n",
    "\n",
    "chatbot_with_message_history = RunnableWithMessageHistory(\n",
    "    chatbot, \n",
    "    get_session_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ef148-1c37-4006-bb02-780d994803bd",
   "metadata": {},
   "source": [
    "#### What is BaseChatMessageHistory and what it does?\n",
    "BaseChatMessageHistory is what is called an **abstract base class** in Python. [See more info about this here](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html). This means it serves as a template or a foundational **blueprint for other classes**. It outlines a set of methods and structures that any class inheriting from it must implement or adhere to, but it cannot be used to create objects directly.\n",
    "\n",
    "Here's a simpler breakdown of what it means for `BaseChatMessageHistory` to be an abstract base class:\n",
    "\n",
    "1. **Blueprint for Other Classes:** It provides a predefined structure that other classes can follow. Think of it like an outline or a checklist for building something; it specifies what needs to be included, but it isn’t the final product.\n",
    "\n",
    "2. **Cannot Create Instances:** You cannot create an instance of an abstract base class. Trying to create an object directly from `BaseChatMessageHistory` would result in an error because it's meant to be a guide, not something to use directly.\n",
    "\n",
    "3. **Requires Implementation:** Any class that inherits from this abstract base class needs to implement specific methods outlined in `BaseChatMessageHistory`, such as methods for adding messages, retrieving messages, and clearing messages. The class sets the rules, and the subclasses need to follow these rules by providing the actual operational details.\n",
    "\n",
    "4. **Purpose in Design:** Using an abstract base class helps ensure consistency and correctness in the implementation of classes that extend it. It's a way to enforce certain functionalities in any subclass, making sure that they all behave as expected without rewriting the same code multiple times.\n",
    "\n",
    "Overall, the concept of an abstract base class is about setting standards and rules, while leaving the specific details of execution to be defined by subclasses that inherit from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c9c5e",
   "metadata": {},
   "source": [
    "#### Let's explain the previous code in simple terms\n",
    "The previous code manages the chatbot's memory of conversations based on session identifiers. Here’s a breakdown of what the different components do:\n",
    "\n",
    "1. **chatbotMemory**:\n",
    "    - `chatbotMemory = {}`: This initializes an empty dictionary where session IDs and their respective chat histories will be stored.\n",
    "\n",
    "2. **get_session_history Function**:\n",
    "    - This function, `get_session_history`, takes a `session_id` as an argument and returns the chat history associated with that session.\n",
    "    - If a chat history for the given `session_id` does not exist in `chatbotMemory`, a new instance of `ChatMessageHistory` is created and assigned to that `session_id` in the dictionary.\n",
    "    - The function ensures that each session has its own unique chat history, stored and retrieved using the session ID.\n",
    "\n",
    "3. **chatbot_with_message_history**:\n",
    "    - `chatbot_with_message_history = RunnableWithMessageHistory(chatbot, get_session_history)`: This line creates an instance of `RunnableWithMessageHistory` using two arguments: `chatbot` and `get_session_history`.\n",
    "    - The `chatbot` is passed along with the `get_session_history` function. This setup integrates the chatbot with the functionality to handle session-specific chat histories, allowing the chatbot to maintain continuity and context in conversations across different sessions.\n",
    "    - **Learn more about RunnableWithMessageHistory** [here](https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/).\n",
    "\n",
    "Overall, the code organizes and manages a chatbot's memory, enabling it to handle different sessions with users effectively by remembering previous messages within each session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98864e16-66bb-49fb-a72f-2ceb5523b361",
   "metadata": {},
   "source": [
    "#### RunnableWithMessageHistory\n",
    "**When invoking a new RunnableWithMessageHistory, we specify the corresponding chat history using a configurable parameter**. Let's say we want to create a chat memory for one user session, let's call it session1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d51d451-ed97-4752-88df-de8072e45f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = {\"configurable\": {\"session_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d3961a-b565-45b5-a45f-2332ab03cd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Red is a vibrant and energetic color that can evoke strong emotions. It's often associated with passion, love, and energy. Many people find it attention-grabbing and stimulating. Do you have a favorite way that you like to incorporate the color red into your life, such as through fashion, home decor, or art?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite color is red.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378a4aae-c392-439d-a7ea-4ae91a677075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your favorite color is red.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d37c98-3883-4210-a162-5302e109e743",
   "metadata": {},
   "source": [
    "## Let's now change the session_id and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b484c1-da0b-4e90-a304-afbbebe63b76",
   "metadata": {},
   "source": [
    "Now let's create a chat memory for another user session, let's call it session2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f82b3e0-7897-4aa8-b554-1985a3af4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = {\"configurable\": {\"session_id\": \"002\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75248e78-2662-4faa-bd41-4aa2b7963d19",
   "metadata": {},
   "source": [
    "If the chatbot is using this new memory for session2, it will not be able to remember anything from the previous conversation in the session1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "882d0fc2-4360-4205-8cc7-bc7275295eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to try and help you, but I'm a large language model, I don't have any information about your personal preferences, including your favorite color. I'm starting from a blank slate with each conversation. Would you like to tell me your favorite color, or would you like to play a game to try and guess it?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ec598-2b9f-4da1-b169-b270f820df9e",
   "metadata": {},
   "source": [
    "## Let's go back to session1 and see if the memory is still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f36c102a-1f06-490f-88b7-cb6e6380d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = {\"configurable\": {\"session_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f61f9a-b8d5-4149-97cd-c7b6c8683bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your favorite color is red.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd5156-977b-42e9-89e9-7797f6895ebc",
   "metadata": {},
   "source": [
    "As we can see, the chatbot is now able to remember the session1 conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037264c-71aa-47f2-9512-e78c12761eaa",
   "metadata": {},
   "source": [
    "## Our ChatBot has session memory now. Let's check if it remembers the conversation from session2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fac2a638-7968-4475-8cf8-9af8440f3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = {\"configurable\": {\"session_id\": \"002\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d367cb8f-1109-45ca-8e7a-1b76a096ec86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Mursalin. Unfortunately, knowing your name doesn't give me any insight into your favorite color. If you'd like to share it with me, I'd be happy to chat about it. Alternatively, I can suggest some fun color-related conversations or games, if you're interested.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is mursalin.\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39ac2ecd-9800-467e-a612-9e3264185875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Mursalin.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c649bdcc-a393-406b-8121-4cd25ae11163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your favorite color is red.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbce7cd-399d-42f3-be81-bff26506cfbc",
   "metadata": {},
   "source": [
    "## Our chatBot now remembers each of our conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39070b3d-5de9-4751-966e-98b17a4db745",
   "metadata": {},
   "source": [
    "## The importance to manage the Conversation History\n",
    "* The memory of a chatbot is included in the context window of the LLM so, if left unmanaged, can potentially overflow it.\n",
    "* **We are now going to learn how to limit the size of the memory of a chatbot**.\n",
    "* First, let's take a look at what is in the memory of our chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09547883-82fc-480a-8f58-fd7fbc007fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'001': InMemoryChatMessageHistory(messages=[HumanMessage(content='My favorite color is red.'), AIMessage(content=\"Red is a vibrant and energetic color that can evoke strong emotions. It's often associated with passion, love, and energy. Many people find it attention-grabbing and stimulating. Do you have a favorite way that you like to incorporate the color red into your life, such as through fashion, home decor, or art?\", response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 41, 'total_tokens': 107, 'completion_time': 0.087887389, 'prompt_time': 0.001930284, 'queue_time': 0.054233446, 'total_time': 0.089817673}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'finish_reason': 'stop', 'logprobs': None}, id='run-68cbf179-c901-4009-b582-3968cc27df7c-0', usage_metadata={'input_tokens': 41, 'output_tokens': 66, 'total_tokens': 107}), HumanMessage(content=\"What's my favorite color?\"), AIMessage(content='Your favorite color is red.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 122, 'total_tokens': 129, 'completion_time': 0.008913733, 'prompt_time': 0.007510371, 'queue_time': 0.050262488, 'total_time': 0.016424104}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee50badd-23e1-41e9-bda3-dc72699fa39e-0', usage_metadata={'input_tokens': 122, 'output_tokens': 7, 'total_tokens': 129}), HumanMessage(content=\"What's my favorite color?\"), AIMessage(content='Your favorite color is red.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 144, 'total_tokens': 151, 'completion_time': 0.009281112, 'prompt_time': 0.016512966, 'queue_time': 0.056389694, 'total_time': 0.025794078}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'finish_reason': 'stop', 'logprobs': None}, id='run-ed1b1fe6-d617-4876-8bbe-1f408bdaf861-0', usage_metadata={'input_tokens': 144, 'output_tokens': 7, 'total_tokens': 151}), HumanMessage(content='What is my favorite color?'), AIMessage(content='Your favorite color is red.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 166, 'total_tokens': 173, 'completion_time': 0.010119305, 'prompt_time': 0.010329582, 'queue_time': 0.05657606, 'total_time': 0.020448887}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'finish_reason': 'stop', 'logprobs': None}, id='run-fee544c7-5ea3-480a-939b-f032739d7a41-0', usage_metadata={'input_tokens': 166, 'output_tokens': 7, 'total_tokens': 173})]), '002': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"What's my favorite color?\"), AIMessage(content=\"I'm happy to try and help you, but I'm a large language model, I don't have any information about your personal preferences, including your favorite color. I'm starting from a blank slate with each conversation. Would you like to tell me your favorite color, or would you like to play a game to try and guess it?\", response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 41, 'total_tokens': 110, 'completion_time': 0.115075573, 'prompt_time': 0.012003981, 'queue_time': 0.051363219, 'total_time': 0.127079554}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-b51837f0-98ec-4351-b40d-d35f8d012698-0', usage_metadata={'input_tokens': 41, 'output_tokens': 69, 'total_tokens': 110}), HumanMessage(content='My name is mursalin.'), AIMessage(content=\"Nice to meet you, Mursalin. Unfortunately, knowing your name doesn't give me any insight into your favorite color. If you'd like to share it with me, I'd be happy to chat about it. Alternatively, I can suggest some fun color-related conversations or games, if you're interested.\", response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 126, 'total_tokens': 189, 'completion_time': 0.11702266, 'prompt_time': 0.007643988, 'queue_time': 0.055594602, 'total_time': 0.124666648}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'finish_reason': 'stop', 'logprobs': None}, id='run-3226a7a8-88b0-4736-be4f-74387dd95249-0', usage_metadata={'input_tokens': 126, 'output_tokens': 63, 'total_tokens': 189}), HumanMessage(content='What is my name?'), AIMessage(content='Your name is Mursalin.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 203, 'total_tokens': 211, 'completion_time': 0.009640914, 'prompt_time': 0.012140539, 'queue_time': 0.053170021, 'total_time': 0.021781453}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'finish_reason': 'stop', 'logprobs': None}, id='run-049f71dc-3ced-4e25-8779-20bf365801e0-0', usage_metadata={'input_tokens': 203, 'output_tokens': 8, 'total_tokens': 211})])}\n"
     ]
    }
   ],
   "source": [
    "print(chatbotMemory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a7229-ea29-468b-93e1-b2b52951b47c",
   "metadata": {},
   "source": [
    "* Now, **let's define a function to limit the number of messages stored in memory and add it to our chain with .assign**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd33a1bb-d3c4-4011-99c7-105e3d7051e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def limited_memory_of_messages(messages, number_of_messages_to_keep=2):\n",
    "    return messages[-number_of_messages_to_keep:]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "limitedMemoryChain = (\n",
    "    RunnablePassthrough.assign(messages=lambda x: limited_memory_of_messages(x[\"messages\"]))\n",
    "    | prompt \n",
    "    | chatbot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff26a10-27aa-48ef-a3f0-867d3f819c79",
   "metadata": {},
   "source": [
    "* The limited_memory_of_messages function allows you to trim the list of stored messages, keeping only a specified number of the latest ones. For example, if you have a long list of messages and you only want to keep the last two, this function will do that for you.\n",
    "* The lambda function works in conjunction with the `limited_memory_of_messages` function. Here’s a simple breakdown:\n",
    "\n",
    "    1. **Lambda Function**: The `lambda` keyword is used to create a small anonymous function in Python. The `lambda` function defined here takes one argument, `x`.\n",
    "\n",
    "    2. **Function Argument**: The argument `x` is expected to be a dictionary that contains a key named `\"messages\"`. The value associated with this key is a list of messages.\n",
    "\n",
    "    3. **Function Body**: The body of the `lambda` function calls the `limited_memory_of_messages` function. It passes the list of messages found in `x[\"messages\"]` to this function.\n",
    "\n",
    "    4. **Default Behavior of limited_memory_of_messages**: Since the `lambda` function does not specify the `number_of_messages_to_keep` parameter when it calls `limited_memory_of_messages`, the latter will default to keeping the last 2 messages from the list (as defined by the earlier function).\n",
    "\n",
    "In essence, the `lambda` function is a shorthand way to apply the `limited_memory_of_messages` function to the message list contained within a dictionary. It automatically trims the list to the last two messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98d2fd-4e32-4c71-a0c0-99701534f551",
   "metadata": {},
   "source": [
    "**Let's now create our new chatbot with limited message history**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e45d4cd1-2165-451d-939f-66f9cc898c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_with_limited_message_history = RunnableWithMessageHistory(\n",
    "    limitedMemoryChain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b39981-02c7-4d10-a4a8-2efbe9bb3c89",
   "metadata": {},
   "source": [
    "## Let's add 2 more messages to the session1 conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76687e6a-2bbf-450f-8310-7011646fea3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vespa scooters are iconic and stylish vehicles that have a distinct charm. They're often associated with Italian design, vintage flair, and a carefree attitude. Many people enjoy riding Vespa scooters for their ease of use, fuel efficiency, and nostalgic appeal. Are you a Vespa enthusiast or do you own one yourself?\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite vehicles are Vespa scooters.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c8a9b0c-7ae5-4f3b-b93b-06462c6d3199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"San Francisco is a vibrant and eclectic city known for its Golden Gate Bridge, steep hills, and diverse neighborhoods. From Fisherman's Wharf to Haight-Ashbury, there's no shortage of culture, history, and natural beauty to explore. The city's iconic landmarks, such as Alcatraz Island and Coit Tower, are a testament to its unique character. What is it about San Francisco that draws you to it?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite city is San Francisco.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c688e6-3e9f-4c11-a4f7-38eb8dd35666",
   "metadata": {},
   "source": [
    "## The chatbot memory has now 4 messages. Let's check the Chatbot with limited memory. \n",
    "* Remember, this chatbot only remembers the last 2 messages, so if we ask her about the first message she should not remember it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16875068-39fd-419e-84b0-f3363f30a3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your personal preferences, including your favorite color. This conversation just started, and I don't have any prior knowledge about you. Would you like to share your favorite color with me?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_limited_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what is my favorite color?\")],\n",
    "    },\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9989ae1-d75b-4b8f-9235-b4eb110e102d",
   "metadata": {},
   "source": [
    "* The chatbot with limited memory has behaved as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324091a-dc49-42af-b638-324dc646cdae",
   "metadata": {},
   "source": [
    "## Finally, let's compare the previous response with the one provided by the Chatbot with unlimited memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "251e1915-dabf-4591-82cc-a0fa8b465ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You mentioned earlier that your favorite color is red.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e34b2-3c42-4ef7-9deb-18c2ec374adc",
   "metadata": {},
   "source": [
    "* As you can see, this chatbot remembers our first message."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
