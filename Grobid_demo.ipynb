{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ea199",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GrobidPDFDocumentLoader' from 'langchain_community.document_loaders' (/home/mursalin/Program/LangChain_tutorials/venv/lib/python3.11/site-packages/langchain_community/document_loaders/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GrobidPDFDocumentLoader\n\u001b[32m      3\u001b[39m loader = GrobidPDFDocumentLoader(\n\u001b[32m      4\u001b[39m     file_path=\u001b[33m\"\u001b[39m\u001b[33m132880L.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     host=\u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:8070\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     grobid_api_process=\u001b[33m\"\u001b[39m\u001b[33mprocessFulltextDocument\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Default processes full document\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m documents = loader.load()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'GrobidPDFDocumentLoader' from 'langchain_community.document_loaders' (/home/mursalin/Program/LangChain_tutorials/venv/lib/python3.11/site-packages/langchain_community/document_loaders/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import \n",
    "\n",
    "loader = GrobidPDFDocumentLoader(\n",
    "    file_path=\"132880L.pdf\",\n",
    "    host=\"http://localhost:8070\",\n",
    "    grobid_api_process=\"processFulltextDocument\"  # Default processes full document\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import GrobidParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0aac97",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m loader = GenericLoader.from_filesystem(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     glob=\u001b[33m\"\u001b[39m\u001b[33m132880L.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     suffixes=[\u001b[33m\"\u001b[39m\u001b[33m.pdf\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     parser=GrobidParser(segment_sentences=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load documents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m docs = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Program/LangChain_tutorials/venv/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:30\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Document]:\n\u001b[32m     29\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Program/LangChain_tutorials/venv/lib/python3.11/site-packages/langchain_community/document_loaders/generic.py:116\u001b[39m, in \u001b[36mGenericLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load documents lazily. Use this when working at a large scale.\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blob_loader.yield_blobs():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blob_parser.lazy_parse(blob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Program/LangChain_tutorials/venv/lib/python3.11/site-packages/langchain_community/document_loaders/parsers/grobid.py:92\u001b[39m, in \u001b[36mGrobidParser.process_xml\u001b[39m\u001b[34m(self, file_path, xml_data, segment_sentences)\u001b[39m\n\u001b[32m     89\u001b[39m         chunks.append(sentence_dict)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m segment_sentences \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     91\u001b[39m     fpage, lpage = (\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[43mchunk_bboxes\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     93\u001b[39m         chunk_bboxes[-\u001b[32m1\u001b[39m][-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     94\u001b[39m     )\n\u001b[32m     95\u001b[39m     paragraph_dict = {\n\u001b[32m     96\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(paragraph_text),\n\u001b[32m     97\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpara\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(i),\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpages\u001b[39m\u001b[33m\"\u001b[39m: (fpage, lpage),\n\u001b[32m    102\u001b[39m     }\n\u001b[32m    103\u001b[39m     chunks.append(paragraph_dict)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the loader for a single PDF file in the current directory\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \".\",\n",
    "    glob=\"132880L.pdf\",\n",
    "    suffixes=[\".pdf\"],\n",
    "    parser=GrobidParser(segment_sentences=False),\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9063ba88",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GrobidParser.__init__() got an unexpected keyword argument 'clean_xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenericLoader\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GrobidParser\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m parser = \u001b[43mGrobidParser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_xml\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m loader = GenericLoader.from_filesystem(\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     glob=\u001b[33m\"\u001b[39m\u001b[33m132880L.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     suffixes=[\u001b[33m\"\u001b[39m\u001b[33m.pdf\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m     parser=parser,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m docs = loader.load()\n",
      "\u001b[31mTypeError\u001b[39m: GrobidParser.__init__() got an unexpected keyword argument 'clean_xml'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import GrobidParser\n",
    "\n",
    "parser = GrobidParser(\n",
    "    segment_sentences=False,\n",
    "    clean_xml=True,\n",
    ")\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \".\",\n",
    "    glob=\"132880L.pdf\",\n",
    "    suffixes=[\".pdf\"],\n",
    "    parser=parser,\n",
    ")\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc5bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import PyPDFParser\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \".\",\n",
    "    glob=\"132880L.pdf\",\n",
    "    suffixes=[\".pdf\"],\n",
    "    parser=PyPDFParser(),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e7d8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n Almost all pattern classification techniques, which often invol ve binarization and tilt correction through image \\npreprocessing, are the fundamental algorithms of traditional Ti betan historical text recognition research. In particular, a \\nbinarization technique based on Lab color space channel combina tion and local processing based on various Tibetan \\nhistorical text backdrop colors was proposed by Han Yuehui et a l. [11]. The same year, Gao Fei et al. introduced an image \\nbinarization approach based on historical book images that have  been damaged. The method divides the picture of the \\ndeteriorated historical book text and backdrop and extracts the  image's crucial value from the damaged historical book \\nimage.This allows for the adaptive compensation of the Tibetan historical book image[12]. For historical Tibetan woodcut \\nimages, a geometrically corrected -based tilt correction approac h has been proposed [13], which is said to be more efficient \\nthan the projection contour method and the traditional Hough me thod. Because of the intricate design of Tibetan \\nhistorical texts, the layout will first need to be separated or  examined in order to accurately extract the text area. \\nPublished research work includes text region localization techn iques based on bounding box correction algorithms and \\nprojection analysis based on connected domains and vertical hor izontals [14]. Using the variations in the projection values \\nof each layout structure element of the image of Tibetan histor ical documents following two segmentations and several \\nprojections, Wang Xiuyou et al. were able to accomplish the tas k of segmenting Tibetan history books [15]. Ren et al. \\nsuggested a layout segmentation approach for Tibetan history bo oks that classifies the feat ure vectors of Tibetan \\nhistorical document images consisting of grayscale, color, and texture data using Support Vector Machines [16]. Text \\nregion in Tibetan historical document picture can be precisely determined through layout analysis; Tibetan characters in \\nthe text area must then be recognized; early text line and sing le character segmentation is required. According to Table 1, \\nnumerous scholars have suggested various techniques for precise ly segmenting Tibetan charact ers because the majority \\nof Tibetan historical records are handwritten and woodcut, whic h results in common character adhesion and overlap \\nbetween the lines. \\nTable 1. Segmentation method of image adhesion text line segmen tation of Tibetan historical documents. \\nMethods Text line segmentation Word segmentation \\nText line baseline estimation or detection[17-24] √ x \\nText Line Baseline Estimation or Detection[25-27] √ √ \\nConnectivity domain analysis[28] √ √ \\nConnected domain anal ysis and A* algorithm[29] √ x \\nConnectivity Domain and Projection Analysis[30] √ x \\nAnalysis of image chunked projection[31] √ x \\nLocal baseline detection and connectivity domain assignment[32] √ x \\nText line outline tracking[33] √ x \\nTable 1 above makes clear that accurately detecting or situatin g identified characters is the fundamental task of character \\nrecognition in Tibetan historical literature, and that inaccura te character detection significantly affects recognition. \\nBaseline estimate, connection domain analysis, and projection f orm the foundation of most text line segmentation \\ntechniques used in Tibetan historical sources. The segmentation  of a single character can b e done flawlessly by vertical \\nprojection as long as the text line is positioned accurately in  the early stages. Zhao Dongcai, in conclusion, presented a \\nBP network-based Tibetan single-character recognition technique  f o r  o l d  T i b e t a n  w o o d c u t s  [34]. Zach Rowinski \\ndeveloped the Namsel OCR system in 2011 [35]. It is capable of reading orthodox Tibetan books and contempor ary \\nprinted Tibetan documents, but it is unable to recognize materi als with adhesive characters,  such as handwritten copies \\nand historical Tibetan woodcuts. The work moved to UC Berkeley in 2013, when Rowinski started collaborating with \\nother academics to further develop the system's capabilities. H e also integrated Heyadati's early work on the hidden \\nMarkov model and actively collabo rated with the Tibetan Buddhis t Resource Center to produce a significant quantity of \\nTibetan electronic texts. Source [36] suggests a technique for recognizing Tibetan characters that d oes not require a lot of \\ntraining samples and is based on  component position information . \\nBased primarily on conventional techniques, character recogniti on research methods in Tibetan historical books \\npublished both domestically and internationally are based on th e body of existing literature. The majority of the central \\nalgorithms in the aforementioned research are conventional patt ern classification techniques , which typically require \\ncutting text lines first, followed by characters, and then forw arding them to a single character recognition module for \\nidentification. Traditional machine learning techniques are use d to investigate it, and it often focuses on topics like \\ncharacter segmentation, feature extraction, and classification algorithms. The early character segmentation benefits and Proc. of SPIE Vol. 13288  132880L-4\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a051b930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '132880L.pdf', 'page': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3].metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
